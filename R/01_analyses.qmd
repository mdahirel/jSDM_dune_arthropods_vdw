---
title: "01 - analyses"
format: html
editor_options: 
  chunk_output_type: console
---

```{r load-packages}
library(ape)
library(coda)
library(conflicted)
library(datawizard)
library(Hmsc)
library(ncf)
library(phytools)
library(posterior)
library(tidybayes)
library(tidyverse)

library(here)
```

```{r load-custom-functions}
HMSCfunctions <- list.files(here("R", "tidyHMSC"), pattern = "*.R")

for (i in 1:length(HMSCfunctions)) {
  source(here("R", "tidyHMSC", HMSCfunctions[i]))
}
```

```{r solve-conflicts}
conflict_scout()
conflicted::conflict_prefer_all("purrr", quiet = TRUE) # avoid a map() conflict with a dependency of phytools
conflicted::conflict_prefer_all("dplyr", quiet = TRUE)
```


# Part 1: Formatting datasets for analyses

```{r load-raw-data}
raw_community <- read_csv(here("data", "Y_long_community.csv"))
raw_trait_data <- read_csv(here("data", "traits.csv"))
raw_envt <- read_csv(here("data", "data_vars.csv"))
raw_tree <- read.tree(here("data", "trees", "tree_relabeled_with_singletons.newick"))
```

## Tree data

Note how the tree reimported from the newick file has underscores instead of spaces between genus and species?

```{r check-tree-tip-names}
raw_tree$tip.label
```

We're leaving that as is **for now** (easier for managing species names as column names in species occurrence data table), so we will need to add the underscores to the species names in both occurrence and trait tables.

- The saved tree has a lot of singletons nodes from the Open Tree of Life. Basically it's all larger clade names from which we pruned all branches but one in the making of our subtree. Since the root of our tree is at the common ancestor of arthropods and molluscs, there can be a lot of these. We kept them in the saved file because they helped make a pretty annotated tree plot, but we need to remove them for the actual analysis here as they may mess up branch length calculations
- Then we add branch length to the OTOL tree. We do that using Grafen's method (see `?compute.brlen()` for reference)

```{r tree-branch-lengths}
phylotree <- raw_tree |>
  collapse.singles() |>
  compute.brlen(method = "Grafen")
```

## Community data

Then we manage the community data:

- we add underscores to `species` names
- we sort data by alphabetical order of species and sites
- we pivot the data to the wide format, one species = one column
- finally, we convert the community data to a matrix, with `sampleID` as row names

```{r make-YData}
community_wide <- raw_community |>
  mutate(species = str_replace_all(species, " ", "_")) |>
  arrange(sample_ID, species) |>
  pivot_wider(names_from = species, values_from = presence)

YData <- community_wide |>
  select(-sample_ID) |>
  as.matrix()

row.names(YData) <- community_wide$sample_ID
# hist(colMeans(YData),50)
```

## Trait data

Now let's move to formatting trait data.
We do a few things:

- first, we add underscores in the `species` names, again
- then we sort species by names
- we create a log10(size) variable
- we scale the continuous variables (size here) to mean 0 and unit 1SD. Strictly speaking not needed, as `Hmsc()` does it internally as a default, but see **Environmental data** below for why it's helpful to do it manually
- we also convert the trophic level variables to factors, and reorder these factors so that the dominant trophic level (herbivore, see below) is the factor reference level
- we finish by moving species names to the data.frame row names

```{r checking-trophic-levels}
table(raw_trait_data$trophic_level_adult)
table(raw_trait_data$trophic_level_larva)
```

```{r make-TrData}
TrData <- raw_trait_data |>
  mutate(species = str_replace_all(species, " ", "_")) |>
  arrange(species) |>
  mutate(log10size = log(size_mm, 10)) |>
  standardize(append = "_scaled") |>
  mutate(trophic_level_adult = fct_relevel(
    trophic_level_adult,
    c("Herbivore", "Predator", "Detritivore", "Omnivore")
  )) |>
  mutate(trophic_level_larva = fct_relevel(
    trophic_level_larva,
    c("Herbivore", "Predator", "Detritivore", "Omnivore")
  )) |>
  as.data.frame()

row.names(TrData) <- TrData$species
```

## Environmental data

```{r check-vitality}
table(raw_envt$vitality)
```

There are 5 vitality levels, but there is only 1 zero value and very few 1 values. In first analyses, vitality was used to fit a continuous quadratic effect. Given the limited range of values and the fact that there are almost no low values, this seems ill-advised. So, if we do want to allow for non-linear effects of vitality, the best solution in my opinion is to use it as a factor. Given there is only 1 zero value, it is also best to pool it with the 1s as "low vitality".

```{r check-districts}
table(raw_envt$district)
```

So, to make the environmental data table:

- we convert vitality values to a factor, with the 0 and 1 pooled. We set highest vitality as the reference level
- we set the district with the most samples (Flemish dunes) as the reference level for `district`, converted to factor
- we scale continuous variables. This has usually two benefits (i) improves Bayesian model fitting performance (ii) makes effects of different variables comparable to each other. As mentioned above, not strictly needed for (i), since `Hmsc()` can do it internally, but `Hmsc()` backtransforms coefficients to original scales in its outputs, so for (ii) we do need to scale variables ourselves)

```{r make-XData}
XData <- raw_envt |>
  arrange(sample_ID) |>
  mutate(vitalityF = case_when(
    vitality < 2 ~ "0_1",
    TRUE ~ as.character(vitality)
  ), .after = vitality) |>
  mutate(vitalityF = fct_relevel(vitalityF, c("4", "3", "2", "0_1"))) |>
  mutate(district = fct_relevel(district, "Flemish dunes", after = 0)) |>
  standardize(append = "_scaled") |>
  mutate(dupe = duplicated(data.frame(X, Y))) |> # 2 samples have same coordinates as 2 other (presumably nearby)
  mutate(
    sample_ID = factor(sample_ID),
    transect = factor(transect),
    country = factor(country)
  ) |> # Hmsc() doesn't like variables as character in this dataframe, even if not used in formula
  as.data.frame()

row.names(XData) <- XData$sample_ID
```

## Final checks

```{r final-data-checks}
levels(factor(XData$district))
levels(factor(XData$vitalityF))
table(rownames(YData) == rownames(XData))
table(rownames(TrData) == colnames(YData))
# everything is properly re-ordered
```

we're ready to make our model, following what's written in the Methods of the main text

# Part 2: the model

## Model formulas

```{r XFormula}
XFormula <- ~ vitalityF + district +
  proportion_50_scaled + I(proportion_50_scaled^2) + mt_stat_50_scaled
```
Since we centred our continuous variables, we are "allowed" to build our quadratic effects "manually" (see Schielzeth 2010 MEE, DOI:10.1111/j.2041-210X.2010.00012.x). Compared to using `poly()`, this will make it easier to interpret them a bit, but more importantly will reduce the need for name cleaning when doing plots later.

```{r TrFormula}
TrFormula <- ~ trophic_level_adult + log10size_scaled
```

## Making the model

```{r random-effects-structure}
studyDesign <- XData |> select(sample_ID)
rSample <- HmscRandomLevel(units = XData$sample_ID) |>
  setPriors(a1 = 60, a2 = 60)
```

The default prior values for `a1` and `a2` are both 50 (see `str(rSample)` before `setPriors()`), setting them higher makes for emptier correlations (stronger shrinkage), see HMSC book. Models with prior values of 50 have major difficulties to converge with this model's structure and our data.

```{r model_full}
if (file.exists(here("outputs", "mfull.RDS"))) {
  mfull <- readRDS(here("outputs", "mfull.RDS"))
} else {
  starttime <- Sys.time()

  mfull <- Hmsc(
    Y = YData,
    XData = XData,
    XFormula = XFormula,
    TrData = TrData,
    TrFormula = TrFormula,
    phyloTree = phylotree,
    studyDesign = studyDesign,
    ranLevels = list("sample_ID" = rSample),
    distr = "probit"
  )

  set.seed(42)
  mfull <- sampleMcmc(mfull,
    nChains = 4,
    thin = 30,
    samples = 1000,
    transient = 15000,
    nParallel = 4
  )
  saveRDS(mfull, file = here("outputs", "mfull.RDS"))

  endtime <- Sys.time()
  endtime - starttime
}
## model takes about 5 hours to run
```

## Basic model diagnostics

```{r HSMCsummary}
if (file.exists(here("outputs", "summary_mfull.RDS"))) {
  summary_mfull <- readRDS(here("outputs", "summary_mfull.RDS"))
} else {
  
summary_mfull <- HMSCsummary(mfull)

  saveRDS(summary_mfull, file = here("outputs", "summary_mfull.RDS"))
}
```

# Part 3: model results

## 3A - Getting some information about predicted species occurrence


```{r HMSCoccurrence}
if (file.exists(here("outputs", "occ_mfull.RDS"))) {
  occ_mfull <- readRDS(here("outputs", "occ_mfull.RDS"))
} else {
  
fits_mfull <- HMSCfitted(mfull)
occ_mfull <- HMSCoccurrence(fits_mfull)
occ_mfull$occ
occ_mfull$occ_avg

saveRDS(occ_mfull, file = here("outputs", "occ_mfull.RDS"))

}
```

## 3B - cross-validation and model performance

### creating cross-validated predictions

As a trade-off between running time and need to have more folds, we use 10-fold cross-validation

```{r making-CVpreds}
if (file.exists(here("outputs", "fits_CVfull.RDS"))) {
  fits_CVfull <- readRDS(here("outputs", "fits_CVfull.RDS"))
} else {
  starttime <- Sys.time()

  set.seed(42)
  partition <- createPartition(mfull, nfolds = 10, column = "sample_ID")
  preds_CVfull <- computePredictedValues(mfull, partition = partition, nParallel = 4)

  attr(preds_CVfull, "HMSCcrossval") <- TRUE

  fits_CVfull <- HMSCfitted_CV(mfull, preds_CVfull)

  saveRDS(fits_CVfull, file = here("outputs", "fits_CVfull.RDS"))

  endtime <- Sys.time()
  endtime - starttime
}
```

```{r HMSCoccurrenceCV}
if (file.exists(here("outputs", "occ_CVfull.RDS"))) {
  occ_CVfull <- readRDS(here("outputs", "occ_CVfull.RDS"))
} else {
  
occ_CVfull <- HMSCoccurrence(fits_CVfull)
occ_CVfull$occ
occ_CVfull$occ_avg

saveRDS(occ_CVfull, file = here("outputs", "occ_CVfull.RDS"))

}
```


### model performance metrics

We compute out-of-sample model performance metrics on the *cross-validated* predictions
```{r perf-metrics}
if (file.exists(here("outputs", "tjur_CVfull.RDS"))) {
  tjur_CVfull <- readRDS(here("outputs", "tjur_CVfull.RDS"))
} else {
  tjur_CVfull <- HMSCtjurD(fits_CVfull)

  saveRDS(tjur_CVfull, file = here("outputs", "tjur_CVfull.RDS"))
}


if (file.exists(here("outputs", "PRAUC_CVfull.RDS"))) {
  PRAUC_CVfull <- readRDS(here("outputs", "PRAUC_CVfull.RDS"))
} else {
  PRAUC_CVfull <- HMSCauc(fits_CVfull, auc_type = "PR")

  saveRDS(PRAUC_CVfull, file = here("outputs", "PRAUC_CVfull.RDS"))
}
```

For comparison, let's also calculate the performance metrics on the training set:

```{r perf-metrics-training}
if (file.exists(here("outputs", "tjur_mfull.RDS"))) {
  tjur_mfull <- readRDS(here("outputs", "tjur_mfull.RDS"))
} else {
  tjur_mfull <- HMSCtjurD(fits_mfull)

  saveRDS(tjur_mfull, file = here("outputs", "tjur_mfull.RDS"))
}


if (file.exists(here("outputs", "PRAUC_mfull.RDS"))) {
  PRAUC_mfull <- readRDS(here("outputs", "PRAUC_mfull.RDS"))
} else {
  PRAUC_mfull <- HMSCauc(fits_mfull, auc_type = "PR")

  saveRDS(PRAUC_mfull, file = here("outputs", "PRAUC_mfull.RDS"))
}
```


## 3C - variance partitioning

`Hmsc` provides a way to partition the variance of the "explained" part into its different components (although there is here no easy way to extract posterior uncertainty). let's get it, clean it, and plot it

```{r getting-varpart}
varpart <- computeVariancePartitioning(mfull)$vals |>
  as.data.frame()

varpart$var <- rownames(varpart)

varpart <- as_tibble(varpart) |>
  pivot_longer(-var, values_to = "VC", names_to = "species") |>
  mutate(species = paste0("*", species, "*") |>
    str_replace("_", " ")) |>
  mutate(var = case_when(
    var == "district" ~ "District",
    var == "mt_stat_50_scaled" ~ "Moran's *I*",
    var == "proportion_50_scaled" ~ "P%",
    var == "I(proportion_50_scaled^2)" ~ "P%^2^",
    var == "vitalityF" ~ "Vitality",
    var == "Random: sample_ID" ~ "Random effects (sample)",
    TRUE ~ var
  )) |>
  mutate(var = fct_relevel(
    var,
    rev(c("Vitality", "Moran's *I*", "P%", "P%^2^", "District", "Random effects (sample)"))
  )) |>
  mutate(VCtype = case_when(
    var %in% c("Moran's *I*", "P%", "P%^2^", "Vitality") ~ "Local environmental variables",
    TRUE ~ var
  )) |>
  mutate(VCtype = fct_relevel(
    VCtype,
    rev(c("Local environmental variables", "District", "Random effects (sample)"))
  ))
```

```{r save-varpart}
if (file.exists(here("outputs", "varpart.RDS"))) {
  varpart <- readRDS(here("outputs", "varpart.RDS"))
} else {
  saveRDS(varpart, file = here("outputs", "varpart.RDS"))
}
```


## 3D - supplementary : spatial correlation in site loadings

We were not able to get to converge a model where the site random effects are fully spatially explicit (following ) (not shown in code).

To nonetheless explore whether there is a spatial signal in the correlation matrix seen above, we extract the site loadings on the underlying latent factors and examine whether they present a spatial pattern.

We first extract the full posterior samples (as opposed to the summaries obtained through `HMSCsummary()` earlier in script):

```{r extract-site-loadings}
HMSCcoda <- convertToCodaObject(mfull)

params <- enframe(HMSCcoda, name = "parameter_type")

Es <- filter(params, parameter_type == "Eta") |> unnest(value)


set.seed(42)

Etas <- Es$value[[1]] |> 
  as_draws_df()|>
  pivot_longer(-c(.iteration, .chain, .draw)) |>
  mutate(variable = str_remove(name, "Eta1\\[")) |>
  mutate(variable = str_remove(variable, "\\]")) |>
  mutate(
    sample_ID = str_split_fixed(variable, pattern = ", factor", 2)[, 1],
    factor = str_split_fixed(variable, pattern = ", factor", 2)[, 2]
  ) |>
  left_join(raw_envt |>
    select(sample_ID, X, Y) |>
    mutate(
      X = X + rnorm(length(X), 0, 10^-15),
      Y = Y + rnorm(length(Y), 0, 10^-15)
    )) |>
  ## there are a few plots in the same transect with identical latlon (I suspect it's very close points and GPS imprecision)
  ## we add some minuscule jitter to avoid issues with 0 values
  group_by(.chain, .iteration, .draw, factor) |>
  nest()
```

We end up with a grouped tibble with a list-column `data` that contains for each posterior sample and each latent factor, a table with the coordinates of each sites and their predicted values on the latent factor

For each posterior sample x latent factor, we then estimate the correlogram (by increments of 5 km)...

```{r make-correlograms}
if (file.exists(here("outputs", "Eta_correlogs.RDS"))) {
  Eta_correlogs <- readRDS(here("outputs", "Eta_correlogs.RDS"))
} else {

  starttime <- Sys.time()
  
Eta_correlogs <- Etas |>
  mutate(correl = map(
    .x = data,
    .f = function(tab = .x) {
      correlogram <- correlog(
        x = tab$X, y = tab$Y,
        z = tab$value,
        resamp = 0,
        latlon = TRUE,
        increment = 5
      )

      output <- tibble(
        meandist = correlogram$mean.of.class,
        correlation = correlogram$correlation,
        npairs = correlogram$n
      )
      return(output)
    }
  ))
endtime <- Sys.time()
endtime - starttime

# takes about 1-2 hours
  saveRDS(Eta_correlogs, file = here("outputs", "Eta_correlogs.RDS"))
}
```
